{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pafy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.models import model_from_json \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to detect the expression and show it along the data passed\n",
    "def pre():\n",
    "        while(True):\n",
    "            ret,img=cap.read()    \n",
    "            \n",
    "            #Converting the input stream to grayscale\n",
    "            gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            #Detecting the faces from the grayscale stream\n",
    "            faces= face_cascade.detectMultiScale(gray,1.3,5)\n",
    "            \n",
    "            #Calssifying the expression on the face and tagging the expression near the face\n",
    "            for(x,y,w,h) in faces:\n",
    "                \n",
    "                #crop the face\n",
    "                face = img[int(y):int(y+h), int(x):int(x+w)] \n",
    "                face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY) \n",
    "                \n",
    "                #resize the face\n",
    "                face = cv2.resize(face, (48, 48))\n",
    "                \n",
    "                #Reshaping and Scaling\n",
    "                img_pixels=image.img_to_array(face)\n",
    "                img_pixels=np.expand_dims(img_pixels, axis=0)\n",
    "                img_pixels /=255\n",
    "                \n",
    "                #Detecting the expression in the face\n",
    "                predictions=model.predict(img_pixels)        \n",
    "                max_index = np.argmax(predictions[0])        \n",
    "                emotion=emotions[max_index]\n",
    "                \n",
    "                #Tagging the expression\n",
    "                cv2.putText(img,emotion,(int(x),int(y)),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
    "            \n",
    "            #Showing the real time feed with the expression tagged along the face\n",
    "            cv2.imshow('img',img)\n",
    "            if (cv2.waitKey(10))==27:\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1 for Live Cam Facial Emotion Detection, 2 for Facial Emotion Detection in a YouTube Video and 3 for a image\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#Loading the model\n",
    "model= model_from_json(open(\"./modeladam.json\",\"r\").read())\n",
    "model.load_weights('./modeladam.h5')\n",
    "\n",
    "emotions = ('angry','disgust','fear','happy','sad','surprise','neutral')\n",
    "\n",
    "face_cascade= cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "\n",
    "#Getting the input from the user\n",
    "inp=int(input(\"Enter 1 for Live Cam Facial Emotion Detection, 2 for Facial Emotion Detection in a YouTube Video and 3 for a image\\r\\n\"))\n",
    "#Function for FED to work on front cam\n",
    "if inp==1:\n",
    "        cap=cv2.VideoCapture(0)\n",
    "        pre()\n",
    "#Function for FED to work on youtube videos\n",
    "elif inp==2:\n",
    "        url=input(\"Enter the link of the YouTube video\\r\\n\")\n",
    "        video=pafy.new(url)\n",
    "        best=video.getbest(preftype=\"mp4\")\n",
    "        cap=cv2.VideoCapture(best.url)\n",
    "        pre()\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "#Function to for FED to work on pics\n",
    "elif inp==3:\n",
    "        filepath=input(\"Enter the local file path of the image\\r\\n\")\n",
    "        img=image.load_img(filepath,grayscale=True,target_size=(48,48))\n",
    "        x=image.img_to_array(img)\n",
    "        x=np.expand_dims(x,axis=0)\n",
    "        x=x/255\n",
    "        detect= model.predict(x)\n",
    "        \n",
    "        y_pos = np.arange(len(emotions))\n",
    "    \n",
    "        plt.bar(y_pos, detect[0], align='center', alpha=0.5)\n",
    "        plt.xticks(y_pos, emotions)\n",
    "        plt.ylabel('percentage')\n",
    "        plt.title('emotion')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        x = np.array(x, 'float32')\n",
    "        x = x.reshape([48, 48]);\n",
    "        x=x*255\n",
    "        plt.gray()\n",
    "        plt.imshow(x)\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "        print(\"Invalid input\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
